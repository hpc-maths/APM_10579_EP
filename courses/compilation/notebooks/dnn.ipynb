{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a2a7cf8d-931f-4521-b62d-01d84d6bfe7a",
   "metadata": {},
   "source": [
    "# Deep Neural Network\n",
    "\n",
    "You see a lot of people around you who are interested in deep neural networks and you think that it might be interesting to start thinking about creating a software that is as flexible as possible and allows novice users to test this kind of methods.\n",
    "\n",
    "You have no previous knowledge and while searching a bit on the internet, you come across this project https://github.com/HyTruongSon/Neural-Network-MNIST-CPP. You say to yourself that this is a good starting point and decide to spend a bit more time on it.\n",
    "\n",
    "We recall here the key elements found in deep neural networks. We will not go into the mathematical details as this is not the purpose of this course.\n",
    "\n",
    "A deep neurl network is composed of an input, an output and several hidden layers.\n",
    "\n",
    "A neuron is illustrated by the following figure\n",
    "\n",
    "![image](./figures/dnn1.png)\n",
    "\n",
    "This figure comes from a CNRS course called fiddle (https://gricad-gitlab.univ-grenoble-alpes.fr/talks/fidle).\n",
    "\n",
    "We can observe that a neuron is made of weights, a bias and an activation function. The activation function can be a sigmoid, reLU, tanh, ...\n",
    "\n",
    "A deep neural network is composed of several hidden layers with several neurons as illustrated in the following figure\n",
    "\n",
    "![image](./figures/dnn2.png)\n",
    "\n",
    "This figure also comes from the CNRS course fiddle."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3fd5b1e-62a3-495b-8bf3-bcddda0272e5",
   "metadata": {},
   "source": [
    "In the following, we will use these notations:\n",
    "\n",
    "- $w^l_{j,i}$ is the weight of the layer $l$ for the neuron $j$ and the input entry $i$.\n",
    "- $z^l_j$ is the aggregation: $\\sum_i x_{i}^l w_{j, i}^l + b_j^l$ where $x_{i}$ is the input.\n",
    "- $\\sigma$ is the activation function. \n",
    "- $a^l_j$ is the output of the neuron $j$ for the layer $l$.\n",
    "- $L$ is the index of the last layer.\n",
    "- $C(a^L, y)$ is the cost function where $a^L$ is the predict value and $y$ is the expected result.\n",
    "\n",
    "The algorithm has three steps:\n",
    "\n",
    "- the forward propagation: for a given input, cross all the layers until the output.\n",
    "- Then using this output, change the weights and biases to minimize the cost function using a descent gradient. This is called backward propagation\n",
    "- iterate until reaching the maximum number of iterations or a given tolerance.\n",
    "\n",
    "The gradient descent can be written as\n",
    "\n",
    "$$\n",
    "w_{j, i}^l = w_{j, i}^l - \\mu \\frac{\\partial C}{\\partial w_{j, i}^l},\n",
    "$$\n",
    "\n",
    "where $\\mu$ is the learning rate.\n",
    "\n",
    "The equations of the backward propagation are\n",
    "\n",
    "- $\\delta^L_j = \\frac{\\partial C}{\\partial a_j^L}\\sigma'(z_j^L)$\n",
    "- $\\delta^l_j = \\sum_i w^{l+1}_{i, j}\\delta^{l+1}_i \\sigma'(z_j^l)$\n",
    "- $\\frac{\\partial C}{\\partial b^l_j} = \\delta_j^l$\n",
    "- $\\frac{\\partial C}{\\partial w^l_{j, i}} = a^{l-1}_i \\delta_j^l$\n",
    "\n",
    "\n",
    "We need to set of datas: datas for training the neural network and datas for testing the final weights and biases.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "612caed6-e369-4bfb-acc8-fb491871f74e",
   "metadata": {},
   "source": [
    "- Read the code https://github.com/HyTruongSon/Neural-Network-MNIST-CPP carefully and try to recognize each element of the algorithm.\n",
    "\n",
    "- Think of a code organization and data structure that offer more flexibility and readability.\n",
    "\n",
    "- Duplicate `step_0` into `step_1` and add all the `CMakeLists.twt` to create a library of `dnn` source files and the executable of the main function\n",
    "\n",
    "- Duplicate `step_1` into `step_2` and implement the following functions\n",
    "    - `forward_propagation`\n",
    "    - `backward_propagation`\n",
    "    - `evaluate`\n",
    "    \n",
    "- How to proceed to have more flexibility in the choice of the activation function ?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bdf1ac9c-cb2d-41b2-9e9b-98de4fcb3784",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "C++14",
   "language": "C++14",
   "name": "xcpp14"
  },
  "language_info": {
   "codemirror_mode": "text/x-c++src",
   "file_extension": ".cpp",
   "mimetype": "text/x-c++src",
   "name": "c++",
   "version": "14"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "state": {},
    "version_major": 2,
    "version_minor": 0
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
